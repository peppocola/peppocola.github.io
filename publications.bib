@inproceedings{Colavito-2020,
  title = {Leveraging {{Textual}} and {{Non-Textual Features}} for {{Documentation Decluttering}}},
  booktitle = {2020 {{IEEE International Conference}} on {{Software Maintenance}} and {{Evolution}} ({{ICSME}})},
  author = {Colavito, Giuseppe and Basile, Pierpaolo and Novielli, Nicole},
  year = {2020},
  month = sep,
  pages = {862--863},
  issn = {2576-3148},
  doi = {10.1109/ICSME46990.2020.00113},
  urldate = {2024-07-07},
  abstract = {This paper describes the participation of a team from the University of Bari in the Decluttering Challenge organized in the scope of the DocGen2 workshop. We propose a supervised approach relying on a minimal set of non-textual features (length, overlapping between the comment text and the source code, code block type, tags, comment type) and classical textual features (bag-of-words). Our system ranked 2nd in the documentation decluttering task.},
  keywords = {Conferences,Documentation,Machine Learning,Software maintenance,Source Code Comments,Task analysis}
}

@inproceedings{Colavito-2023a,
  title = {Few-{{Shot Learning}} for {{Issue Report Classification}}},
  booktitle = {2023 {{IEEE}}/{{ACM}} 2nd {{International Workshop}} on {{Natural Language-Based Software Engineering}} ({{NLBSE}})},
  author = {Colavito, Giuseppe and Lanubile, Filippo and Novielli, Nicole},
  year = {2023},
  month = may,
  pages = {16--19},
  doi = {10.1109/NLBSE59153.2023.00011},
  urldate = {2024-07-07},
  abstract = {We describe our participation in the tool competition in the scope of the 2nd International Workshop on Natural Language-based Software Engineering. We propose a supervised approach relying on SETFIT, a framework for few-shot learning and sentence-BERT (SBERT), a variant of BERT for effective sentence embedding. We experimented with different settings, achieving the best performance by training and testing the SETFIT-based model on a subset of data with manually verified labels (Fl-micro =.8321). For the sake of the challenge, we evaluate the SETFIT model on the challenge test set, achieving Fl-micro =.7767.},
  keywords = {BERT,Conferences,Data models,deep-learning,few-shot-learning,Issue-classification,labeling-unstructured-data,Manuals,Semantics,software-maintenance-and-evolution,Supervised learning,Training,Training data}
}

@inproceedings{Colavito-2023b,
  title = {Issue Report Classification Using Pre-Trained Language Models},
  booktitle = {Proceedings of the 1st {{International Workshop}} on {{Natural Language-based Software Engineering}}},
  author = {Colavito, Giuseppe and Lanubile, Filippo and Novielli, Nicole},
  year = {2023},
  month = feb,
  series = {{{NLBSE}} '22},
  pages = {29--32},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3528588.3528659},
  urldate = {2024-07-07},
  abstract = {This paper describes our participation in the tool competition organized in the scope of the 1st International Workshop on Natural Language-based Software Engineering. We propose a supervised approach relying on fine-tuned BERT-based language models for the automatic classification of GitHub issues. We experimented with different pre-trained models, achieving the best performance with fine-tuned RoBERTa (F1 = .8591).},
  isbn = {978-1-4503-9343-0}
}

@article{Colavito-2024a,
  title = {Impact of Data Quality for Automatic Issue Classification Using Pre-Trained Language Models},
  author = {Colavito, Giuseppe and Lanubile, Filippo and Novielli, Nicole and Quaranta, Luigi},
  year = {2024},
  month = apr,
  journal = {Journal of Systems and Software},
  volume = {210},
  pages = {111838},
  issn = {0164-1212},
  doi = {10.1016/j.jss.2023.111838},
  urldate = {2024-07-07},
  abstract = {Issue classification aims to recognize whether an issue reports a bug, a request for enhancement or support. In this paper we use pre-trained models for the automatic classification of issues and investigate how the quality of data affects the performance of classifiers. Despite the application of data quality filters, none of our attempts had a significant effect on model quality. As root cause we identify a threat to construct validity underlying the issue labeling. Editor's note: Open Science material was validated by the Journal of Systems and Software Open Science Board.},
  keywords = {BERT,GitHub,Issue labeling,Issue trackers,Label correctness,Model quality},
}

@inproceedings{Colavito-2024b,
  title = {Leveraging {{GPT-like LLMs}} to {{Automate Issue Labeling}}},
  booktitle = {2024 {{IEEE}}/{{ACM}} 21st {{International Conference}} on {{Mining Software Repositories}} ({{MSR}})},
  author = {Colavito, Giuseppe and Lanubile, Filippo and Novielli, Nicole and Quaranta, Luigi},
  year = {2024},
  month = apr,
  pages = {469--480},
  issn = {2574-3864},
  urldate = {2024-07-07},
  abstract = {Issue labeling is a crucial task for the effective management of software projects. To date, several approaches have been put forth for the automatic assignment of labels to issue reports. In particular, supervised approaches based on the fine-tuning of BERT-like language models have been proposed, achieving state-of-the-art performance. More recently, decoder-only models such as GPT have become prominent in SE research due to their surprising capabilities to achieve state-of-the-art performance even for tasks they have not been trained for. To the best of our knowledge, GPT-like models have not been applied yet to the problem of issue classification, despite the promising results achieved for many other software engineering tasks. In this paper, we investigate to what extent we can leverage GPT-like LLMs to automate the issue labeling task. Our results demonstrate the ability of GPT-like models to correctly classify issue reports in the absence of labeled data that would be required to fine-tune BERT-like LLMs.CCS CONCEPTS{$\bullet$} Software and its engineering {$\rightarrow$} Documentation; Software evolution; Maintaining software; {$\bullet$} Information systems {$\rightarrow$} Clustering and classification;},
  keywords = {Annotations,Computational modeling,Data models,GPT,Issue Labeling,Labeling Unstructured Data,LLM,Manuals,Scalability,Software,Software Maintenance and Evolution,Supervised learning}
}

@inproceedings{Novielli-2024,
  title = {{{QualAI}}: {{Continuous Quality Improvement}} of {{AI-based Systems}}.},
  shorttitle = {{{QualAI}}},
  booktitle = {{{RCIS Workshops}}},
  author = {Novielli, Nicole and Oliveto, Rocco and Palomba, Fabio and Calefato, Fabio and Colavito, Giuseppe and De Martino, Vincenzo and Della Porta, Antonio and Giordano, Giammaria and Guglielmi, Emanuela and Lanubile, Filippo},
  year = {2024},
  urldate = {2024-07-15},
}

@inproceedings{Polignano-2022,
  title = {Lexicon {{Enriched Hybrid Hate Speech Detection}} with {{Human-Centered Explanations}}},
  booktitle = {Adjunct {{Proceedings}} of the 30th {{ACM Conference}} on {{User Modeling}}, {{Adaptation}} and {{Personalization}}},
  author = {Polignano, Marco and Colavito, Giuseppe and Musto, Cataldo and De Gemmis, Marco and Semeraro, Giovanni},
  year = {2022},
  month = jul,
  pages = {184--191},
  publisher = {ACM},
  address = {Barcelona Spain},
  doi = {10.1145/3511047.3537688},
  urldate = {2024-07-07},
  isbn = {978-1-4503-9232-7},
  langid = {english},
}

@inproceedings{Kallis_2024,
	title        = {The NLBSE'24 Tool Competition},
	author       = {Kallis, Rafael and Colavito, Giuseppe and Al-Kaswan, Ali and Pascarella, Luca and Chaparro, Oscar and Rani, Pooja},
	year         = 2024,
	booktitle    = {Proceedings of the Third ACM/IEEE International Workshop on NL-Based Software Engineering},
	location     = {Lisbon, Portugal},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {NLBSE '24},
	pages        = {33â€“40},
	doi          = {10.1145/3643787.3648038},
	isbn         = 9798400705762,
	url          = {https://doi.org/10.1145/3643787.3648038},
	abstract     = {We report on the organization and results of the tool competition of the third International Workshop on Natural Language-based Software Engineering (NLBSE'24). As in prior editions, we organized the competition on automated issue report classification, with focus on small repositories, and on automated code comment classification, with a larger dataset. In this tool competition edition, six teams submitted multiple classification models to automatically classify issue reports and code comments. The submitted models were fine-tuned and evaluated on a benchmark dataset of 3 thousand issue reports or 82 thousand code comments, respectively. This paper reports details of the competition, including the rules, the teams and contestant models, and the ranking of models based on their average classification performance across issue report and code comment types.},
	numpages     = 8,
	keywords     = {tool-competition, labeling, benchmark, issue reports, code comments}
}

@article{Colavito_2025a,
	title        = {Benchmarking large language models for automated labeling: The case of issue report classification},
	author       = {Giuseppe Colavito and Filippo Lanubile and Nicole Novielli},
	year         = 2025,
	journal      = {Information and Software Technology},
	volume       = 184,
	pages        = 107758,
	doi          = {https://doi.org/10.1016/j.infsof.2025.107758},
	issn         = {0950-5849},
	url          = {https://www.sciencedirect.com/science/article/pii/S0950584925000977},
	keywords     = {Issue labeling, Generative AI, Software maintenance and evolution},
	abstract     = {Context: Issue labeling is a fundamental task for software development as it is critical for the effective management of software projects. This practice involves assigning a label to issues, such as bug or feature request, denoting a task relevant to the project management. To date, large language models (LLMs) have been proposed to automate this task, including both fine-tuned BERT-like models and zero-shot GPT-like models. Objectives: In this paper, we investigate which LLMs offer the best trade-off between performance, response time, hardware requirements, and quality of the responses for issue report classification. Methods: We design and execute a comprehensive benchmark study to assess 22 generative decoder-only LLMs and 2 baseline BERT-like encoder-only models, which we evaluate on two different datasets of GitHub issues. Results: Generative LLMs demonstrate potential for zero-shot classification. However, their performance varies significantly across datasets and they require substantial computational resources for deployment. In contrast, BERT-like models show more consistent performance and lower resource requirements. Conclusions: Based on the empirical evidence provided in this study, we discuss implications for researchers and practitioners. In particular, our results suggest that fine-tuning BERT-like encoder-only models enables achieving consistent, state-of-the-art performance across datasets even in presence of a small amount of labeled data available for training.}
}

@inproceedings{Novielli_2024,
	title        = {Continuous Quality Improvement of AI-based Systems: the QualAI Project},
	author       = {Novielli, Nicole and Oliveto, Rocco and Palomba, Fabio and Calefato, Fabio and Colavito, Giuseppe and De Martino, Vincenzo and Della Porta, Antonio and Giordano, Giammaria and Guglielmi, Emanuela and Lanubile, Filippo and Quaranta, Luigi and Recupito, Gilberto and Scalabrino, Simone and Spina, Angelica and Vitale, Antonio},
	year         = 2024,
	booktitle    = {Proceedings of the 18th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement},
	location     = {Barcelona, Spain},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {ESEM '24},
	pages        = {603â€“607},
	doi          = {10.1145/3674805.3695393},
	isbn         = 9798400710476,
	url          = {https://doi.org/10.1145/3674805.3695393},
	abstract     = {QualAI is a two-year project aimed at defining a set of recommenders to continuously monitor, assess, and improve the quality of AI-based systems, with a particular focus on machine learning (ML) applications. We will develop recommenders for the quality assurance of both data and ML models to enable practitioners to mitigate technical debt. Special attention will be paid to communication challenges that may arise in hybrid teams comprising data scientists and software developers. This paper presents the project outline, provides an executive summary of the research activities, outlines the expected project outcomes, and reports the results obtained to date.},
	numpages     = 5,
	keywords     = {Machine Learning, Quality Assurance, Recommender Systems, Software Engineering}
}

